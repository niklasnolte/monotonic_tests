{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import glob\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from loaders.blog_loader import load_data, mono_list\n",
    "from monotonenorm import SigmaNet, GroupSort, direct_norm\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Ytr, Xts, Yts = load_data(get_categorical_info=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "monotone_constraints = [1 if i in mono_list else 0 for i in range(Xtr.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=5,\n",
       "              monotone_constraints=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                                    0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...],\n",
       "              n_estimators=10000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = lgb.LGBMRegressor(n_estimators=10000, max_depth=5, learning_rate=.1, monotone_constraints=monotone_constraints)\n",
    "clf.fit(Xtr, Ytr, early_stopping_rounds=200, eval_set=[(Xts, Yts)], eval_metric='rmse', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.14963566230704012, 0.14832811996053064)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse_tr = (((clf.predict(Xtr) - Ytr)**2).mean())**.5\n",
    "rmse_ts = (((clf.predict(Xts) - Yts)**2).mean())**.5\n",
    "rmse_tr, rmse_ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtrt = torch.tensor(Xtr, dtype=torch.float32).cuda()\n",
    "Ytrt = torch.tensor(Ytr, dtype=torch.float32).view(-1, 1).cuda()\n",
    "Xtst = torch.tensor(Xts, dtype=torch.float32).cuda()\n",
    "Ytst = torch.tensor(Yts, dtype=torch.float32).view(-1, 1).cuda()\n",
    "#std = 1\n",
    "mean = Xtrt.mean(0)\n",
    "std = Xtrt.std(0)\n",
    "\n",
    "Xtrt = (Xtrt - mean) / std\n",
    "Xtst = (Xtst - mean) / std\n",
    "\n",
    "\n",
    "def run_exp(seed):\n",
    "  torch.manual_seed(seed)\n",
    "  dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Xtrt, Ytrt), batch_size=256, shuffle=True)\n",
    "\n",
    "  per_layer_lip = .5\n",
    "  width = 4\n",
    "\n",
    "  class Model(torch.nn.Module):\n",
    "    def __init__(self, robust=False, sigma=False):\n",
    "      super().__init__()\n",
    "      if robust:\n",
    "        from monotonenorm import direct_norm\n",
    "        activation = lambda : GroupSort(1)\n",
    "      else:\n",
    "        direct_norm = lambda x, *args, **kwargs: x # make it a normal network\n",
    "        activation = lambda : GroupSort(1)\n",
    "        #activation = lambda : torch.nn.ReLU()\n",
    "        #swish\n",
    "        #activation = lambda : torch.nn.SiLU()\n",
    "\n",
    "      self.nn = torch.nn.Sequential(\n",
    "        direct_norm(torch.nn.Linear(Xtr.shape[1], width), kind=\"one-inf\", alpha=per_layer_lip),\n",
    "        activation(),\n",
    "        direct_norm(torch.nn.Linear(width, width), kind=\"inf\", alpha=per_layer_lip),\n",
    "        activation(),\n",
    "        direct_norm(torch.nn.Linear(width, width), kind=\"inf\", alpha=per_layer_lip),\n",
    "        activation(),\n",
    "        direct_norm(torch.nn.Linear(width, 1), kind=\"inf\", alpha=per_layer_lip),\n",
    "        #direct_norm(mup.MuReadout(width, 1), kind=\"inf\", alpha=per_layer_lip),\n",
    "      )\n",
    "      if sigma:\n",
    "        self.nn = SigmaNet(self.nn, sigma=per_layer_lip**5, monotone_constraints= monotone_constraints)\n",
    "    \n",
    "    def forward(self, x):\n",
    "      return self.nn(x)\n",
    "\n",
    "  model = Model(robust=True, sigma=True).cuda()\n",
    "  print(model)\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "  EPOCHS = 200\n",
    "  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=3e-3, steps_per_epoch=len(dataloader), epochs=EPOCHS)\n",
    "  print('params:', sum(p.numel() for p in model.parameters()))\n",
    "  bar = tqdm(range(EPOCHS))\n",
    "  best_rmse = 1\n",
    "  for i in bar:\n",
    "    for Xi, yi in dataloader:\n",
    "      y_pred = model(Xi)\n",
    "      losstr = torch.nn.functional.mse_loss(y_pred, yi)\n",
    "      optimizer.zero_grad()\n",
    "      losstr.backward()\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "    with torch.no_grad():\n",
    "      y_predts = model(Xtst)\n",
    "      lossts = torch.nn.functional.mse_loss(y_predts, Ytst)\n",
    "      tsrmse = lossts.item()**.5\n",
    "      trrmse = losstr.item()**.5\n",
    "      best_rmse = min(best_rmse, tsrmse)\n",
    "      bar.set_description(f\"train rmse: {trrmse:.4f} test rmse: {tsrmse:.4f}, best: {best_rmse:.4f}, lr: {scheduler.get_last_lr()[0]:.4f}\")\n",
    "  return best_rmse\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (nn): SigmaNet(\n",
      "    (nn): Sequential(\n",
      "      (0): Linear(in_features=276, out_features=4, bias=True)\n",
      "      (1): GroupSort(num_groups: {self.n_groups})\n",
      "      (2): Linear(in_features=4, out_features=4, bias=True)\n",
      "      (3): GroupSort(num_groups: {self.n_groups})\n",
      "      (4): Linear(in_features=4, out_features=4, bias=True)\n",
      "      (5): GroupSort(num_groups: {self.n_groups})\n",
      "      (6): Linear(in_features=4, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "params: 1153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train rmse: 0.1277 test rmse: 0.1685, best: 0.1677, lr: 0.0000: 100%|██████████| 200/200 [02:02<00:00,  1.63it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.1677173962770335"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run_exp(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f66b475c7f3c7893d2633464dcd70b93c7a03e563e2680bf00b3479051dc13c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
