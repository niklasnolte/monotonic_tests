{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from loaders.lending_loader import load_data, mono_list\n",
    "from monotonenorm import SigmaNet, GroupSort\n",
    "from sklearn.metrics import balanced_accuracy_score, accuracy_score\n",
    "import numpy as np\n",
    "import mup\n",
    "\n",
    "device = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr, Ytr, Xts, Yts = load_data(get_categorical_info=False)\n",
    "monotonic_constraints = [int(i in mono_list) for i in range(Xtr.shape[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xtr = Xtr[:30000]\n",
    "Ytr = Ytr[:30000]\n",
    "#Xts = Xts[:1000]\n",
    "#Yts = Yts[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=5,\n",
       "              monotone_constraint=[1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "                                   0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "              n_estimators=10000)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = lgb.LGBMRegressor(n_estimators=10000, max_depth=5, learning_rate=.1, monotone_constraint=monotonic_constraints)\n",
    "clf.fit(Xtr, Ytr, early_stopping_rounds=200, eval_set=[(Xts, Yts)], eval_metric='mse', verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.648450407337777, 0.649028777843047)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc, bacc = 0,0\n",
    "for i in np.linspace(0,1,50):\n",
    "  acc = max(acc, accuracy_score(Yts, clf.predict(Xts)>i))\n",
    "  bacc = max(bacc, balanced_accuracy_score(Yts, clf.predict(Xts)>i))\n",
    "acc, bacc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "params: 10241\n"
     ]
    }
   ],
   "source": [
    "\n",
    "per_layer_lip = 2\n",
    "width = 64\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "  def __init__(self, width, robust=False, sigma=False):\n",
    "    super().__init__()\n",
    "    if robust:\n",
    "      from monotonenorm import direct_norm\n",
    "      activation = lambda : GroupSort(2)\n",
    "    else:\n",
    "      direct_norm = lambda x, *args, **kwargs: x # make it a normal network\n",
    "      activation = lambda : torch.nn.ReLU()\n",
    "\n",
    "    self.nn = torch.nn.Sequential(\n",
    "      direct_norm(torch.nn.Linear(Xtr.shape[1], width), kind=\"one-inf\", alpha=per_layer_lip),\n",
    "      activation(),\n",
    "      direct_norm(torch.nn.Linear(width, width), kind=\"inf\", alpha=per_layer_lip),\n",
    "      activation(),\n",
    "      direct_norm(torch.nn.Linear(width, width), kind=\"inf\", alpha=per_layer_lip),\n",
    "      activation(),\n",
    "      direct_norm(mup.MuReadout(width, 1), kind=\"inf\", alpha=per_layer_lip),\n",
    "    )\n",
    "    if sigma:\n",
    "      self.nn = SigmaNet(self.nn, sigma=per_layer_lip**4, monotone_constraints=monotonic_constraints)\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.nn(x)\n",
    "\n",
    "base = Model(1)\n",
    "delta = Model(2)\n",
    "model = Model(width)\n",
    "mup.set_base_shapes(model, base, delta=delta)\n",
    "\n",
    "for param in model.parameters():\n",
    "    ### If initializing manually with fixed std or bounds,\n",
    "    ### then replace with same function from mup.init\n",
    "    # torch.nn.init.uniform_(param, -0.1, 0.1)\n",
    "    mup.init.uniform_(param, -0.1, 0.1)\n",
    "    ### Likewise, if using\n",
    "    ###   `xavier_uniform_, xavier_normal_, kaiming_uniform_, kaiming_normal_`\n",
    "    ### from `torch.nn.init`, replace with the same functions from `mup.init`\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer = mup.MuAdam(model.parameters(), lr=1e-3)\n",
    "print('params:', sum(p.numel() for p in model.parameters()))\n",
    "#scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=.999)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (nn): Sequential(\n",
      "    (0): Linear(in_features=28, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): MuReadout(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6385 0.6236, acc: 0.6402: 100%|██████████| 1000/1000 [07:42<00:00,  2.16it/s]\n"
     ]
    }
   ],
   "source": [
    "print(model)\n",
    "Xtrt = torch.tensor(Xtr, dtype=torch.float32).to(device)\n",
    "Ytrt = torch.tensor(Ytr, dtype=torch.float32).view(-1, 1).to(device)\n",
    "Xtst = torch.tensor(Xts, dtype=torch.float32).to(device)\n",
    "Ytst = torch.tensor(Yts, dtype=torch.float32).view(-1, 1).to(device)\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(torch.utils.data.TensorDataset(Xtrt, Ytrt), batch_size=int(2**7), shuffle=True)\n",
    "\n",
    "bar = tqdm(range(1000))\n",
    "for i in bar:\n",
    "  for Xi, yi in dataloader:\n",
    "    y_pred = model(Xi)\n",
    "    losstr = torch.nn.functional.binary_cross_entropy_with_logits(y_pred, yi)\n",
    "    optimizer.zero_grad()\n",
    "    losstr.backward()\n",
    "    optimizer.step()\n",
    "    #scheduler.step()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    y_predts = model(Xtst)\n",
    "    lossts = torch.nn.functional.binary_cross_entropy_with_logits(y_predts, Ytst)\n",
    "    if i % 10 == 0:\n",
    "      acc = 0\n",
    "      for i in np.linspace(.3, .7, 50):\n",
    "        acc = max(acc, accuracy_score(Ytst.cpu().numpy(), y_predts.cpu().numpy()>i))\n",
    "    bar.set_description(f'Loss: {losstr.item():.4f} {lossts.item():.4f}, acc: {acc.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9f66b475c7f3c7893d2633464dcd70b93c7a03e563e2680bf00b3479051dc13c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
